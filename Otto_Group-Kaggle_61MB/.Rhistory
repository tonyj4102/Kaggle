require(xgboost)
## Loading required package: xgboost
require(methods)
## Loading required package: methods
require(data.table)
## Loading required package: data.table
require(magrittr)
## Loading required package: magrittr
install.packages("data.table")
setwd("~/Dropbox")
setwd("~/Dropbox/Projects/OttoGroup-Kaggle")
train <- fread('train.csv', header = T, stringsAsFactors = F)
require(data.table)
train <- fread('train.csv', header = T, stringsAsFactors = F)
test <- fread('test.csv', header=TRUE, stringsAsFactors = F)
dim(train)
test[1:6,1:5, with =F]
# Delete ID column in training dataset
train[, id := NULL]
# Delete ID column in testing dataset
test[, id := NULL]
According to its description, the Otto challenge is a multi class classification challenge. We need to extract the labels (here the name of the different classes) from the dataset. We only have two files (test and training), it seems logical that the training file contains the class we are looking for. Usually the labels is in the first or the last column. We already know what is in the first column, let’s check the content of the last one.
# Delete ID column in testing dataset
test[, id := NULL]
According to its description, the Otto challenge is a multi class classification challenge. We need to extract the labels (here the name of the different classes) from the dataset. We only have two files (test and training), it seems logical that the training file contains the class we are looking for. Usually the labels is in the first or the last column. We already know what is in the first column, let’s check the content of the last one.
# Check the content of the last column
train[1:6, ncol(train), with  = F]
nameLastCol <- names(train)[ncol(train)]
The classes are provided as character string in the 94th column called target. As you may know, XGBoost doesn’t support anything else than numbers. So we will convert classes to integers. Moreover, according to the documentation, it should start at 0.
train[, nameLastCol:=NULL, with = F]
trainMatrix <- train[,lapply(.SD,as.numeric)] %>% as.matrix
testMatrix <- test[,lapply(.SD,as.numeric)] %>% as.matrix
numberOfClasses <- max(y) + 1
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
numberOfClasses <- max(y) + 1
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}
y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}
train[1:6, ncol(train), with  = F]
nameLastCol <- names(train)[ncol(train)]
nameLastCol
train[1:6, ncol(train), with  = F]
train <- fread('train.csv', header = T, stringsAsFactors = F)
test <- fread('test.csv', header=TRUE, stringsAsFactors = F)
dim(train)
train[1:6,1:5, with =F]
dim(train)
test[1:6,1:5, with =F]
train[, id := NULL]
test[, id := NULL]
train[1:6, ncol(train), with  = F]
nameLastCol <- names(train)[ncol(train)]
nameLastCol
y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}
[1:5]
[1:5]
y[1:5]
train[, nameLastCol:=NULL, with = F]
trainMatrix <- train[,lapply(.SD,as.numeric)] %>% as.matrix
testMatrix <- test[,lapply(.SD,as.numeric)] %>% as.matrix
numberOfClasses <- max(y) + 1
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
cv.nround <- 5
cv.nfold <- 3
bst.cv = xgb.cv(param=param, data = trainMatrix, label = y,
nfold = cv.nfold, nrounds = cv.nround)
require(xgboost)
## Loading required package: methods
install.packages("xgboost)
install.packages("xgboost")
require(xgboost)
cv.nround <- 5
cv.nfold <- 3
bst.cv = xgb.cv(param=param, data = trainMatrix, label = y,
nfold = cv.nfold, nrounds = cv.nround)
nround = 50
bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)
Plot(log(bst.cv$test.logloss.mean),type=”l”)
Plot(log(bst.cv$test.logloss.mean),type="l")
plot(log(bst.cv$test.logloss.mean),type="l")
plot(log(bst.cv$test.mlogloss.mean),type="l")
nround = 50
bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)
Preds=predict(bst,trainMatrix)
Preds[1:10]
Print(-mean(log(preds)*test$label+log(1-preds)*(1-testlabel)))
print(-mean(log(preds)*test$label+log(1-preds)*(1-testlabel)))
preds=predict(bst,trainMatrix)
print(-mean(log(preds)*test$label+log(1-preds)*(1-testlabel)))
print(-mean(log(preds)*test$label+log(1-preds)*(1-test$label)))
print(-mean(log(preds)*test$target+log(1-preds)*(1-test$target)))
str(test)
(dimnames(train)[[2]]
dimnames(train)[[2]]
dimnames(train)[[2]]
Trees = xgb.model.dt.tree(dimnames(train$names)[[2]], model = bst)
View(Trees)
Names <- dimnames(train)[[2]]
Importance_matrix<-xgb.importance(names,model =bst)
names <- dimnames(train)[[2]]
Importance_matrix<-xgb.importance(names,model =bst)
Xgb.plot.importance(importance_matrix[1:10])
xgb.plot.importance(importance_matrix[1:10])
xgb.plot.importance(Importance_matrix[1:10])
install.packages("Ckmeans.1d.dp")
library(Ckmeans.1d.dp)
xgb.plot.importance(Importance_matrix[1:10])
Xgb.plot.tree(feature_name = names,model =bst, n_first_tree = 2)
xgb.plot.tree(feature_name = names,model =bst, n_first_tree = 2)
install.packages("DiagrammeR")
library("DiagrammeR")
xgb.plot.tree(feature_name = names,model =bst, n_first_tree = 2)
xgb.plot.tree(feature_name = names,model =bst, n_first_tree = 2)
model <- xgb.dump(bst, with.stats = T)
model[1:10]
